"""
RAG Retriever Node - X·ª≠ l√Ω c√¢u h·ªèi ki·∫øn th·ª©c t·ªïng qu√°t v·ªõi PDF retrieval.

Module n√†y x·ª≠ l√Ω c√°c c√¢u h·ªèi KH√îNG c·∫ßn SQL:
1. Load v√† index PDF documents v√†o ChromaDB
2. Retrieve relevant chunks b·∫±ng semantic search
3. Tr·∫£ l·ªùi b·∫±ng LLM v·ªõi context t·ª´ documents

Dependencies:
- chromadb: Vector database
- sentence-transformers: Embedding model
- pypdf: PDF parsing
- langchain-text-splitters: Text chunking
"""

import os
import re
import hashlib
from typing import Dict, Any, List, Optional
from pathlib import Path

from dotenv import load_dotenv
import google.generativeai as google_genai

# Load environment
load_dotenv()
if os.getenv("GOOGLE_API_KEY") in (None, "") and os.getenv("GEMINI_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

# ========== CONFIGURATION ==========
# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a PDF documents
DOCUMENTS_DIR = Path(__file__).parent.parent / "data" / "documents"
# ƒê∆∞·ªùng d·∫´n l∆∞u ChromaDB
CHROMA_PERSIST_DIR = Path(__file__).parent.parent / "data" / "chroma_db"
# Collection name trong ChromaDB
COLLECTION_NAME = "djia_documents"
# Chunk size v√† overlap cho text splitting
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200


def get_chroma_client():
    """
    Kh·ªüi t·∫°o ChromaDB client v·ªõi persistent storage.
    """
    try:
        import chromadb
        from chromadb.config import Settings
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        CHROMA_PERSIST_DIR.mkdir(parents=True, exist_ok=True)
        
        client = chromadb.PersistentClient(
            path=str(CHROMA_PERSIST_DIR),
            settings=Settings(anonymized_telemetry=False)
        )
        return client
    except ImportError:
        print("ChromaDB not installed. Run: pip install chromadb")
        return None


def get_embedding_function():
    """
    L·∫•y embedding function s·ª≠ d·ª•ng sentence-transformers.
    Model: all-MiniLM-L6-v2 (nh·∫π, nhanh, hi·ªáu qu·∫£)
    """
    try:
        from chromadb.utils import embedding_functions
        
        return embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
    except ImportError:
        print("sentence-transformers not installed. Run: pip install sentence-transformers")
        return None


def load_pdf(file_path: Path) -> str:
    """
    Load v√† extract text t·ª´ PDF file.
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n t·ªõi file PDF
        
    Returns:
        Text content c·ªßa PDF
    """
    try:
        from pypdf import PdfReader
        
        reader = PdfReader(str(file_path))
        text_parts = []
        
        for page in reader.pages:
            text = page.extract_text()
            if text:
                text_parts.append(text)
        
        return "\n\n".join(text_parts)
    except ImportError:
        print("pypdf not installed. Run: pip install pypdf")
        return ""
    except Exception as e:
        print(f"Error loading PDF {file_path}: {e}")
        return ""


def split_text(text: str, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP) -> List[str]:
    """
    Split text th√†nh c√°c chunks nh·ªè h∆°n ƒë·ªÉ indexing.
    
    Args:
        text: Text c·∫ßn split
        chunk_size: K√≠ch th∆∞·ªõc m·ªói chunk (characters)
        chunk_overlap: ƒê·ªô overlap gi·ªØa c√°c chunks
        
    Returns:
        List c√°c text chunks
    """
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
            length_function=len,
        )
        
        chunks = splitter.split_text(text)
        return chunks
    except ImportError:
        # Fallback: simple splitting
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start = end - chunk_overlap
        return chunks


def compute_file_hash(file_path: Path) -> str:
    """T√≠nh MD5 hash c·ªßa file ƒë·ªÉ detect changes."""
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()


def index_documents(force_reindex: bool = False) -> bool:
    """
    Index t·∫•t c·∫£ PDF documents trong th∆∞ m·ª•c v√†o ChromaDB.
    
    Args:
        force_reindex: True ƒë·ªÉ reindex to√†n b·ªô, b·ªè qua cache
        
    Returns:
        True n·∫øu c√≥ documents ƒë∆∞·ª£c index
    """
    client = get_chroma_client()
    if not client:
        return False
    
    embedding_fn = get_embedding_function()
    if not embedding_fn:
        return False
    
    # Get or create collection
    try:
        if force_reindex:
            # X√≥a collection c≈© n·∫øu force reindex
            try:
                client.delete_collection(COLLECTION_NAME)
            except:
                pass
        
        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_fn,
            metadata={"hnsw:space": "cosine"}
        )
    except Exception as e:
        print(f"Error creating collection: {e}")
        return False
    
    # T√¨m t·∫•t c·∫£ PDF files
    if not DOCUMENTS_DIR.exists():
        DOCUMENTS_DIR.mkdir(parents=True, exist_ok=True)
        print(f"Created documents directory: {DOCUMENTS_DIR}")
        return False
    
    pdf_files = list(DOCUMENTS_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"No PDF files found in {DOCUMENTS_DIR}")
        return False
    
    # Index t·ª´ng file
    indexed_count = 0
    for pdf_path in pdf_files:
        file_hash = compute_file_hash(pdf_path)
        doc_id_prefix = f"{pdf_path.stem}_{file_hash[:8]}"
        
        # Check if already indexed (by checking if any document with this prefix exists)
        existing = collection.get(where={"source": str(pdf_path.name)})
        if existing and existing["ids"] and not force_reindex:
            # Check hash to see if file changed
            existing_hash = existing["metadatas"][0].get("file_hash", "") if existing["metadatas"] else ""
            if existing_hash == file_hash:
                print(f"Skipping {pdf_path.name} (already indexed)")
                continue
            else:
                # File changed, delete old entries
                collection.delete(where={"source": str(pdf_path.name)})
        
        print(f"Indexing {pdf_path.name}...")
        
        # Load v√† split PDF
        text = load_pdf(pdf_path)
        if not text:
            print(f"  Warning: No text extracted from {pdf_path.name}")
            continue
        
        chunks = split_text(text)
        print(f"  Split into {len(chunks)} chunks")
        
        # Add to collection
        ids = [f"{doc_id_prefix}_{i}" for i in range(len(chunks))]
        metadatas = [
            {
                "source": str(pdf_path.name),
                "file_hash": file_hash,
                "chunk_index": i,
                "total_chunks": len(chunks),
            }
            for i in range(len(chunks))
        ]
        
        # Add in batches to avoid memory issues
        batch_size = 100
        for i in range(0, len(chunks), batch_size):
            batch_end = min(i + batch_size, len(chunks))
            collection.add(
                ids=ids[i:batch_end],
                documents=chunks[i:batch_end],
                metadatas=metadatas[i:batch_end],
            )
        
        indexed_count += 1
        print(f"  ‚úì Indexed {len(chunks)} chunks from {pdf_path.name}")
    
    print(f"\nTotal: Indexed {indexed_count} documents, Collection size: {collection.count()}")
    return indexed_count > 0 or collection.count() > 0


def retrieve_from_db(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Retrieve relevant chunks t·ª´ ChromaDB.
    
    Args:
        query: C√¢u h·ªèi c·∫ßn t√¨m context
        top_k: S·ªë l∆∞·ª£ng chunks tr·∫£ v·ªÅ
        
    Returns:
        List c√°c documents v·ªõi metadata
    """
    client = get_chroma_client()
    if not client:
        return []
    
    embedding_fn = get_embedding_function()
    if not embedding_fn:
        return []
    
    try:
        collection = client.get_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_fn
        )
        
        if collection.count() == 0:
            print("Warning: ChromaDB collection is empty. Run index_documents() first.")
            return []
        
        # Query v·ªõi semantic search
        results = collection.query(
            query_texts=[query],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        
        # Format results
        documents = []
        if results and results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                metadata = results["metadatas"][0][i] if results["metadatas"] else {}
                distance = results["distances"][0][i] if results["distances"] else 0
                
                documents.append({
                    "content": doc,
                    "source": metadata.get("source", "unknown"),
                    "chunk_index": metadata.get("chunk_index", 0),
                    "distance": distance,  # Lower = more similar (cosine distance)
                    "relevance_score": 1 - distance,  # Convert to similarity score
                })
        
        return documents
    
    except Exception as e:
        print(f"Error retrieving from ChromaDB: {e}")
        return []


def detect_general_question(question: str) -> bool:
    """
    Ph√°t hi·ªán xem c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn ki·∫øn th·ª©c trong PDF documents kh√¥ng.
    
    S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch c√¢u h·ªèi v√† x√°c ƒë·ªãnh:
    - N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn ki·∫øn th·ª©c t·ªïng qu√°t, kh√°i ni·ªám, gi·∫£i th√≠ch c√≥ th·ªÉ t√¨m trong PDF ‚Üí True
    - N·∫øu c√¢u h·ªèi y√™u c·∫ßu d·ªØ li·ªáu c·ª• th·ªÉ t·ª´ database (gi√°, volume, s·ªë li·ªáu) ‚Üí False
    
    Returns:
        True n·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn PDF documents v√† n√™n d√πng RAG
        False n·∫øu c√¢u h·ªèi c·∫ßn truy v·∫•n SQL database
    """
    if not question or not question.strip():
        return False
    
    api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        # Fallback: n·∫øu kh√¥ng c√≥ API key, tr·∫£ v·ªÅ False ƒë·ªÉ d√πng SQL pipeline
        print("Warning: No Gemini API key found. Skipping RAG detection.")
        return False
    
    try:
        google_genai.configure(api_key=api_key)
        
        # Prompt ƒë·ªÉ LLM ph√¢n t√≠ch c√¢u h·ªèi
        prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch c√¢u h·ªèi cho h·ªá th·ªëng t√†i ch√≠nh.

H·ªá th·ªëng c√≥ 2 ngu·ªìn th√¥ng tin:
1. PDF Documents: Ch·ª©a ki·∫øn th·ª©c t·ªïng qu√°t v·ªÅ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n, DJIA, kh√°i ni·ªám, gi·∫£i th√≠ch, ƒë·ªãnh nghƒ©a
2. SQL Database: Ch·ª©a d·ªØ li·ªáu c·ª• th·ªÉ v·ªÅ gi√° c·ªï phi·∫øu, volume, ng√†y th√°ng c·ª• th·ªÉ c·ªßa c√°c c√¥ng ty

C√ÇU H·ªéI: {question}

NHI·ªÜM V·ª§: X√°c ƒë·ªãnh xem c√¢u h·ªèi n√†y c√≥ li√™n quan ƒë·∫øn ki·∫øn th·ª©c trong PDF documents kh√¥ng.

QUY T·∫ÆC:
- Tr·∫£ v·ªÅ TRUE n·∫øu c√¢u h·ªèi h·ªèi v·ªÅ: kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, gi·∫£i th√≠ch, ki·∫øn th·ª©c t·ªïng qu√°t, l√Ω thuy·∫øt, c√°ch th·ª©c ho·∫°t ƒë·ªông
  V√≠ d·ª•: "What is DJIA?", "Gi·∫£i th√≠ch v·ªÅ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n", "How does stock market work?"
  
- Tr·∫£ v·ªÅ FALSE n·∫øu c√¢u h·ªèi y√™u c·∫ßu: d·ªØ li·ªáu c·ª• th·ªÉ, s·ªë li·ªáu, gi√° c·∫£, volume, so s√°nh s·ªë li·ªáu, bi·ªÉu ƒë·ªì d·ªØ li·ªáu
  V√≠ d·ª•: "What was the price of Apple on 2024-01-15?", "Plot the volume", "Compare prices of AAPL and MSFT"

CH·ªà TR·∫¢ L·ªúI: TRUE ho·∫∑c FALSE (kh√¥ng c√≥ d·∫•u ch·∫•m, kh√¥ng c√≥ gi·∫£i th√≠ch th√™m)"""

        model = google_genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        
        result = (response.text or "").strip().upper()
        
        # Parse k·∫øt qu·∫£
        if "TRUE" in result:
            return True
        elif "FALSE" in result:
            return False
        else:
            # N·∫øu LLM tr·∫£ v·ªÅ format kh√¥ng chu·∫©n, m·∫∑c ƒë·ªãnh l√† False
            print(f"Warning: LLM returned unexpected format: {result}. Defaulting to False.")
            return False
            
    except Exception as e:
        print(f"Error in detect_general_question with LLM: {e}")
        # Fallback: n·∫øu c√≥ l·ªói, tr·∫£ v·ªÅ False ƒë·ªÉ d√πng SQL pipeline
        return False


def answer_with_context(question: str, context_docs: List[Dict[str, Any]]) -> str:
    """
    Tr·∫£ l·ªùi c√¢u h·ªèi s·ª≠ d·ª•ng LLM v·ªõi context t·ª´ RAG.
    """
    api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        return "Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi LLM. Vui l√≤ng ki·ªÉm tra API key."
    
    google_genai.configure(api_key=api_key)
    
    # Build context string v·ªõi source attribution
    if context_docs:
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            source = doc.get("source", "unknown")
            content = doc.get("content", "")
            relevance = doc.get("relevance_score", 0)
            context_parts.append(
                f"[Document {i}] (Source: {source}, Relevance: {relevance:.2f})\n{content}"
            )
        context_text = "\n\n---\n\n".join(context_parts)
    else:
        context_text = "Kh√¥ng t√¨m th·∫•y context ph√π h·ª£p trong knowledge base."
    
    # Detect language
    vietnamese_chars = re.findall(
        r'[√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', 
        question.lower()
    )
    is_vietnamese = len(vietnamese_chars) > 0
    
    system_prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n v√† DJIA (Dow Jones Industrial Average).

RETRIEVED CONTEXT FROM DOCUMENTS:
{context_text}

Y√äU C·∫¶U:
- Tr·∫£ l·ªùi d·ª±a tr√™n context ƒë∆∞·ª£c cung c·∫•p t·ª´ documents
- N·∫øu context kh√¥ng ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ v√† cung c·∫•p ki·∫øn th·ª©c chung
- {'Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát' if is_vietnamese else 'Answer in English'}
- Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, d·ªÖ hi·ªÉu
- Cite source documents khi c√≥ th·ªÉ (v√≠ d·ª•: "Theo t√†i li·ªáu X...")
- N·∫øu c√¢u h·ªèi v·ªÅ gi√° c·ª• th·ªÉ c·ªßa m·ªôt c·ªï phi·∫øu t·∫°i th·ªùi ƒëi·ªÉm n√†o ƒë√≥, h√£y h∆∞·ªõng d·∫´n ng∆∞·ªùi d√πng h·ªèi l·∫°i v·ªõi format ph√π h·ª£p"""

    prompt = f"{system_prompt}\n\nC√¢u h·ªèi: {question}\n\nTr·∫£ l·ªùi:"
    
    try:
        model = google_genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        answer = (response.text or "").strip()
        return answer if answer else "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."
    except Exception as e:
        return f"L·ªói khi g·ªçi LLM: {str(e)}"


def rag_retrieve(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LangGraph Node: RAG Retriever - T√¨m ki·∫øm th√¥ng tin t·ª´ PDF documents.
    
    Node n√†y ƒë∆∞·ª£c g·ªçi khi c√¢u h·ªèi KH√îNG li√™n quan SQL (ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i b·ªüi question_classifier).
    - T√¨m ki·∫øm th√¥ng tin li√™n quan t·ª´ PDF documents
    - N·∫øu c√≥ th√¥ng tin ph√π h·ª£p: Tr·∫£ v·ªÅ context ƒë·ªÉ g·ª≠i cho answer_summarizer
    - N·∫øu kh√¥ng c√≥: Tr·∫£ v·ªÅ flag kh√¥ng c√≥ th√¥ng tin trong PDF
    
    Args:
        state: Dictionary ch·ª©a workflow state, c·∫ßn c√≥ key "question"
        
    Returns:
        State m·ªõi v·ªõi c√°c key:
        - has_rag_context: Boolean - C√≥ th√¥ng tin li√™n quan trong PDF kh√¥ng
        - rag_context: List documents retrieved (n·∫øu c√≥)
    """
    question = state.get("question", "")
    
    # ƒê·∫£m b·∫£o documents ƒë√£ ƒë∆∞·ª£c index
    index_documents(force_reindex=False)
    
    # Retrieve context t·ª´ ChromaDB
    context_docs = retrieve_from_db(question, top_k=5)
    
    if not context_docs:
        # Kh√¥ng c√≥ documents trong knowledge base
        return {
            **state,
            "has_rag_context": False,
            "rag_context": [],
        }
    
    # Ki·ªÉm tra relevance score c·ªßa documents
    # N·∫øu t·∫•t c·∫£ documents c√≥ relevance score th·∫•p (< 0.3), coi nh∆∞ kh√¥ng ph√π h·ª£p
    min_relevance = 0.3
    relevant_docs = [doc for doc in context_docs if doc.get("relevance_score", 0) >= min_relevance]
    
    if not relevant_docs:
        # Documents kh√¥ng ph√π h·ª£p v·ªõi c√¢u h·ªèi
        return {
            **state,
            "has_rag_context": False,
            "rag_context": [],
        }
    
    # C√≥ documents ph√π h·ª£p, tr·∫£ v·ªÅ context ƒë·ªÉ answer_summarizer x·ª≠ l√Ω
    return {
        **state,
        "has_rag_context": True,
        "rag_context": relevant_docs,
    }


# ========== UTILITY FUNCTIONS ==========

def reindex_all():
    """
    Force reindex t·∫•t c·∫£ documents.
    G·ªçi h√†m n√†y khi c√≥ PDF m·ªõi ho·∫∑c c·∫ßn rebuild index.
    
    Usage:
        from nodes.rag_retriever import reindex_all
        reindex_all()
    """
    print("Force reindexing all documents...")
    index_documents(force_reindex=True)


def get_collection_info() -> Dict[str, Any]:
    """
    L·∫•y th√¥ng tin v·ªÅ ChromaDB collection.
    
    Returns:
        Dictionary ch·ª©a th√¥ng tin collection
    """
    client = get_chroma_client()
    if not client:
        return {"error": "ChromaDB not available"}
    
    try:
        embedding_fn = get_embedding_function()
        collection = client.get_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_fn
        )
        
        # Get sample of documents
        sample = collection.peek(limit=3)
        
        return {
            "name": COLLECTION_NAME,
            "count": collection.count(),
            "sample_sources": list(set(
                m.get("source", "unknown") 
                for m in sample.get("metadatas", [])
            )) if sample.get("metadatas") else [],
        }
    except Exception as e:
        return {"error": str(e)}


def test_retrieval(query: str, top_k: int = 3):
    """
    Test retrieval v·ªõi m·ªôt query.
    
    Usage:
        from nodes.rag_retriever import test_retrieval
        test_retrieval("What is DJIA?")
    """
    print(f"\nüîç Query: {query}")
    print("-" * 50)
    
    results = retrieve_from_db(query, top_k)
    
    if not results:
        print("No results found. Make sure documents are indexed.")
        return
    
    for i, doc in enumerate(results, 1):
        print(f"\nüìÑ Result {i}:")
        print(f"   Source: {doc['source']}")
        print(f"   Relevance: {doc['relevance_score']:.3f}")
        print(f"   Content: {doc['content'][:200]}...")


if __name__ == "__main__":
    # Test script
    print("=" * 60)
    print("RAG Retriever - PDF Document Search")
    print("=" * 60)
    
    # Index documents
    print("\nüìö Indexing documents...")
    index_documents()
    
    # Show collection info
    print("\nüìä Collection info:")
    info = get_collection_info()
    print(f"   Documents: {info.get('count', 0)}")
    print(f"   Sources: {info.get('sample_sources', [])}")
    
    # Test query
    test_retrieval("What is DJIA?")
